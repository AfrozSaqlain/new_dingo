# datasets
waveform_dataset_path: /Users/mdax/Documents/dingo/devel/dingo-devel/tutorials/02_gwpe/datasets/waveforms/03_IMR_test/waveform_dataset.hdf5
asd_dataset_path: /Users/mdax/Documents/dingo/devel/data/PSDs/asds_O1.hdf5

# data conditioning for inference
data_conditioning:
  frequency_range: [20, 1024]
  window_kwargs:
    window_type: tukey
    f_s: 4096
    T: 8.0
    roll_off: 0.4
#  0.9374713897717841

# settings for transforms
transform_settings:
  detectors:
    - H1
    - L1
  extrinsic_prior:
    dec: default
    ra: default
    geocent_time: bilby.core.prior.Uniform(minimum=-0.03, maximum=0.03)
    psi: default
    luminosity_distance: bilby.core.prior.Uniform(minimum=100.0, maximum=1000.0)
  ref_time: 1126259462.391
  selected_parameters: default
#    - chirp_mass
#    - mass_ratio
#    - luminosity_distance
#    - dec

# model architecture
model_arch:
  # model builder for nde with enet
  model_builder:
    dingo.core.nn.nsf.create_nsf_with_rb_projection_embedding_net
  # kwargs for model_builder
  model_kwargs:
    # kwargs for neural spline flow
    nsf_kwargs:
      num_flow_steps: 3 # 30
      base_transform_kwargs:
        hidden_dim: 64 # 512
        num_transform_blocks: 5
        activation: elu
        dropout_probability: 0.0
        batch_norm: True
        num_bins: 8
        base_transform_type: rq-coupling
    # kwargs for embedding net
    embedding_net_kwargs:
      n_rb: 200
      V_rb_list: null
      output_dim: 128
#      hidden_dims: [1024, 1024, 1024, 1024, 1024, 1024,
#                    512, 512, 512, 512, 512, 512,
#                    256, 256, 256, 256, 256, 256,
#                    128, 128, 128, 128, 128, 128]
      hidden_dims: [1024, 512, 256, 128]
      activation: elu
      dropout: 0.0
      batch_norm: True
      # added_context = True for concatenation of GNPE proxies to enet output
      added_context: False # True

# settings for training
train_settings:
  # fraction of dataset used for training, rest is used for validation
  train_fraction: 0.001
  batch_size: 64
  num_workers: 0 # num_workers >0 does not work on Mac, see https://stackoverflow.com/questions/64772335/pytorch-w-parallelnative-cpp206
  optimizer_kwargs:
    type: adam
    lr: 0.0001
  scheduler_kwargs:
    type: cosine
    T_max: 300
  runtime_limits:
    max_time_per_run: 36000
    max_epochs_per_run: 5
    max_epochs_total: 300
  checkpoint_epochs: 3