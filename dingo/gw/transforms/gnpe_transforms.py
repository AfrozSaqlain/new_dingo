import numpy as np
import torch
import lal
from bilby.core.prior import PriorDict
from abc import ABC, abstractmethod

from dingo.gw.domains import FrequencyDomain


class GNPEBase(ABC):
    """
    A base class for Group Equivariant Neural Posterior Estimation [1].

    This implements GNPE for *approximate* equivariances. For exact equivariances,
    additional processing should be implemented within a subclass.

    [1]: https://arxiv.org/abs/2111.13139
    """

    def __init__(self, kernel_dict, operators):
        self.kernel = PriorDict(kernel_dict)
        self.operators = operators
        self.proxy_list = [k + "_proxy" for k in kernel_dict.keys()]

    @abstractmethod
    def __call__(self, input_sample):
        pass

    def sample_proxies(self, input_parameters):
        """
        Given input parameters, perturbs based on the
        kernel to produce "proxy" ("hatted") parameters, i.e., samples

            \hat g ~ p(\hat g | g).

        Typically the GNPE NDE will be conditioned on \hat g. Furthermore, these proxy
        parameters will be used to transform the data to simplify it.

        Parameters:
        -----------
        input_parameters : dict
            Initial parameter values to be perturbed. dict values can be either floats
            (for training) or torch Tensors (for inference).

        Returns
        -------
        A dict of proxy parameters.
        """
        proxies = {}
        for k in self.kernel:
            if k not in input_parameters:
                raise KeyError(
                    f"Input parameters are missing key {k} required for GNPE."
                )
            g = input_parameters[k]
            g_hat = self.perturb(g, k)
            proxies[k + "_proxy"] = g_hat
        return proxies

    def perturb(self, g, k):
        """
        Generate proxy variables based on initial parameter values.

        Parameters
        ----------
        g : Union[np.float64, float, torch.Tensor]
            Initial parameter values
        k : str
            Parameter name. This is used to identify the group binary operator.

        Returns
        -------
        Proxy variables in the same format as g.
        """
        # First we sample from the kernel, ensuring the correct data type,
        # and accounting for possible batching.
        #
        # Batching is implemented only for torch Tensors (expected at inference time),
        # whereas un-batched data in float form is expected during training.
        if type(g) == torch.Tensor:
            epsilon = self.kernel[k].sample(len(g))
            epsilon = torch.tensor(epsilon, dtype=g.dtype, device=g.device)
        elif type(g) == np.float64 or type(g) == float:
            epsilon = self.kernel[k].sample()
        else:
            raise NotImplementedError(f"Unsupported data type {type(g)}.")

        return self.multiply(g, epsilon, k)

    def multiply(self, a, b, k):
        op = self.operators[k]
        if op == "+":
            return a + b
        elif op == "x":
            return a * b
        else:
            raise NotImplementedError(
                f"Unsupported group multiplication operator: {op}"
            )

    def inverse(self, a, k):
        op = self.operators[k]
        if op == "+":
            return -a
        elif op == "x":
            return 1 / a
        else:
            raise NotImplementedError(
                f"Unsupported group multiplication operator: {op}"
            )


class GNPECoalescenceTimes(GNPEBase):
    """
    GNPE [1] Transformation for detector coalescence times.

    For each of the detector coalescence times, a proxy is generated by adding a
    perturbation epsilon from the GNPE kernel to the true detector time. This proxy is
    subtracted from the detector time, such that the overall time shift only amounts to
    -epsilon in training. This standardizes the input data to the inference network,
    since the applied time shifts are always restricted to the range of the kernel.

    To preserve information at inference time, conditioning of the inference network on
    the proxies is required. To that end, the proxies are stored in sample[
    'gnpe_proxies'].

    We can enforce an exact equivariance under global time translations, by subtracting
    one proxy (by convention: the first one, usually for H1 ifo) from all other
    proxies, and from the geocent time, see [1]. This is enabled with the flag
    exact_global_equivariance.

    Note that this transform does not modify the data itself. It only determines the
    amount by which to time-shift the data.

    [1]: arxiv.org/abs/2111.13139
    """

    def __init__(
        self, ifo_list, kernel, exact_global_equivariance=True, inference=False
    ):
        """
        Parameters
        ----------
        ifo_list : bilby.gw.detector.InterferometerList
            List of interferometers.
        kernel : str
            Defines a Bilby prior, to be used for all interferometers.
        exact_global_equivariance : bool = True
            Whether to impose the exact global time translation symmetry.
        inference : bool = False
            Whether to use inference or training mode.
        """
        self.ifo_time_labels = [ifo.name + "_time" for ifo in ifo_list]
        kernel_dict = {k: kernel for k in self.ifo_time_labels}
        operators = {k: "+" for k in self.ifo_time_labels}
        super().__init__(kernel_dict, operators)

        self.inference = inference
        self.exact_global_equivariance = exact_global_equivariance
        if self.exact_global_equivariance:
            del self.proxy_list[0]

    def __call__(self, input_sample):
        sample = input_sample.copy()
        extrinsic_parameters = sample["extrinsic_parameters"].copy()
        new_parameters = self.sample_proxies(extrinsic_parameters)

        # If we are in training mode, we assume that the time shifting due to different
        # arrival times of the signal in individual detectors has not yet been applied
        # to the data; instead the arrival times are stored in extrinsic_parameters.
        # Hence we subtract off the proxy times from these arrival times, so that time
        # shifting of the data only has to be done once.
        if not self.inference:
            for k in self.ifo_time_labels:
                new_parameters[k] = (
                    -new_parameters[k + "_proxy"] + extrinsic_parameters[k]
                )
        # In inference mode, the data are only time shifted by minus the proxy.
        else:
            for k in self.ifo_time_labels:
                new_parameters[k] = -new_parameters[k + "_proxy"]

        # If we are imposing the global time shift symmetry, then we treat the first
        # proxy as "preferred", in the sense that it defines the global time shift.
        # This symmetry is enforced as follows:
        #
        #    1) Do not explicitly condition the model on the preferred proxy
        #    2) Subtract the preferred proxy from geocent_time (assumed to be a regression
        #    parameter). Note that this must be undone at inference time.
        #    3) Subtract the preferred proxy from the remaining proxies. These remaining
        #    proxies then define time shifts relative to the global time shift.
        #
        # Imposing the global time shift does not impact the transformation of the
        # data: we do not change the values of the true detector coalescence times
        # stored in extrinsic_parameters, only the proxies.
        if self.exact_global_equivariance:
            dt = new_parameters.pop(self.ifo_time_labels[0] + "_proxy")
            if not self.inference:
                if "geocent_time" not in extrinsic_parameters:
                    raise KeyError(
                        "geocent_time should be in extrinsic_parameters at "
                        "this point during training."
                    )
                new_parameters["geocent_time"] = (
                    extrinsic_parameters["geocent_time"] - dt
                )
            else:
                new_parameters["geocent_time"] = -dt
            for k in self.ifo_time_labels[1:]:
                new_parameters[k + "_proxy"] -= dt

        extrinsic_parameters.update(new_parameters)
        sample["extrinsic_parameters"] = extrinsic_parameters
        return sample


class GNPEChirpMass(GNPEBase):
    def __init__(self, kernel, domain):
        kernel_dict = {"chirp_mass": kernel}
        operators = {"chirp_mass": "x"}
        super().__init__(kernel_dict, operators)
        self.domain = domain

    def __call__(self, input_sample):
        sample = input_sample.copy()
        extrinsic_parameters = sample["extrinsic_parameters"].copy()
        proxies = self.sample_proxies(sample["parameters"])
        extrinsic_parameters.update(proxies)
        sample["extrinsic_parameters"] = extrinsic_parameters

        # The only situation where we would expect to not have a waveform to transform
        # would be when calculating parameter standardizations, since we just want to
        # draw samples of the parameters at that point, and not prepare any data.
        if "waveform" in sample:
            sample["waveform"] = self.factor_fiducial_waveform(
                proxies["chirp_mass_proxy"], sample["waveform"]
            )

        return sample

    def factor_fiducial_waveform(self, chirp_mass, data):
        if isinstance(self.domain, FrequencyDomain):
            if type(data) == dict:
                f = self.domain.get_sample_frequencies_astype(list(data.values())[0])
            else:
                f = self.domain.get_sample_frequencies_astype(data)

            # Expand across possible batch dimension.
            if type(chirp_mass) == np.float64 or type(chirp_mass) == float:
                mf = chirp_mass * f
                # Avoid taking a negative power of 0 in the first index. This will get
                # chopped off or multiplied by 0 later anyway.
                if f[0] == 0.0:
                    mf[0] = 1.0
            elif type(chirp_mass) == torch.Tensor:
                mf = torch.outer(chirp_mass, f)
                if f[0] == 0.0:
                    mf[0] = 0.0
            else:
                raise TypeError(
                    f"Invalid type {type(chirp_mass)}. "
                    f"Only implemented for floats and tensors"
                )

            # Add higher order corrections to the leading PN order if desired.
            fiducial_phase = (3 / 128) * (
                np.pi * mf * lal.GMSUN_SI / lal.C_SI ** 3
            ) ** (-5 / 3)

            if type(data) == dict:
                result = {}
                for k, v in data.items():
                    result[k] = self.domain.add_phase(v, -fiducial_phase)
            else:
                result = self.domain.add_phase(data, -fiducial_phase)

            return result

        else:
            raise NotImplementedError("Can only use GNPEChirpMass in frequency domain.")


class GNPEChirpMassOld(object):
    """
    GNPE [1] Transformation for chirp mass.

    Todo

    [1]: arxiv.org/abs/2111.13139
    """

    def __init__(self, frequencies, kernel_kwargs):
        """
        :param frequencies: np.array
            sample frequencies of strain data
        :param kernel_kwargs: dict
            kwargs for gnpe kernel
        :param mean: float = 0
            mean for standardization of proxy
        :param std: float = 1
            standard deviation for standardization of proxy
        """
        self.f = frequencies
        self.kernel = get_gnpe_kernel(kernel_kwargs)

    def __call__(self, input_sample):
        sample = input_sample.copy()
        # Copy extrinsic parameters to not overwrite input_sample. Does this really
        # matter?
        extrinsic_parameters = sample["extrinsic_parameters"].copy()

        # get proxy by adding perturbation from kernel to Mc
        Mc_hat = sample["parameters"]["chirp_mass"] + self.kernel()
        # convert to SI units
        Mc_SI_hat = Mc_hat * lal.GMSUN_SI

        rescaling = np.exp(
            1j
            * (3 / 4)
            * (8 * np.pi * self.f * (Mc_SI_hat / lal.C_SI ** 3)) ** (-5 / 3)
        )
        hc = sample["waveform"]["h_cross"] * rescaling
        hp = sample["waveform"]["h_plus"] * rescaling
        sample["waveform"] = {"h_cross": hc, "h_plus": hp}

        extrinsic_parameters.update({"chirp_mass_proxy": Mc_hat})
        sample["extrinsic_parameters"] = extrinsic_parameters

        # proxies_array = (np.array([Mc_hat]) - self.mean) / self.std
        # if "gnpe_proxies" in sample:
        #     sample["gnpe_proxies"] = np.concatenate(
        #         (sample["gnpe_proxies"], proxies_array)
        #     )
        # else:
        #     sample["gnpe_proxies"] = proxies_array
        return sample
